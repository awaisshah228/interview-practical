{
  "id": "batch_processing",
  "title": "Batch Processing",
  "group": "AI Automation",
  "content": [
    { "type": "h1", "text": "Batch Processing for AI Video" },
    { "type": "image", "src": "https://placehold.co/800x450/1a1a2e/04AA6D?text=Batch+Processing&font=raleway", "alt": "Batch processing diagram showing multiple videos being generated simultaneously", "caption": "Batch processing generates multiple pieces of content in parallel, dramatically increasing throughput" },
    { "type": "h2", "text": "What Is Batch Processing?" },
    { "type": "p", "text": "Batch processing is the technique of <strong>submitting multiple generation requests at once</strong> instead of processing them one at a time. Rather than generating images, video clips, or voiceovers sequentially, you queue up all requests and let them run in parallel or in optimized batches." },
    { "type": "p", "text": "In the context of AI video production, batch processing applies to every stage of the pipeline: generating 50 scripts at once, creating 200 images in a single run, producing voiceovers for an entire week of content, or rendering 30 video clips simultaneously." },
    { "type": "note", "text": "Batch processing is not just about speed — it is also about cost. Many AI APIs offer significant discounts for batch requests (OpenAI's Batch API charges 50% less than real-time requests)." },
    { "type": "h2", "text": "Batch Image Generation" },
    { "type": "p", "text": "Images are typically the easiest asset to batch-generate because image APIs are fast and relatively inexpensive. The key challenge is maintaining <strong>visual consistency</strong> across a large batch." },
    { "type": "table", "rows": [
      ["Tool", "Batch Method", "Max Batch Size", "Avg Time Per Image", "Cost Per Image"],
      ["DALL-E 3 API", "Loop API calls with async/await", "Unlimited (rate limited)", "10-15 seconds", "$0.04 - $0.08"],
      ["Midjourney API", "/imagine batch with --repeat flag", "Up to 40 per batch", "30-60 seconds", "$0.01 - $0.03"],
      ["Stability AI API", "POST /v1/generation with batch param", "10 per request", "5-10 seconds", "$0.002 - $0.006"],
      ["Leonardo AI API", "Batch generation endpoint", "8 per request", "8-15 seconds", "$0.01 - $0.02"],
      ["Flux (Replicate)", "Prediction batch endpoint", "Unlimited (queued)", "5-15 seconds", "$0.003 - $0.01"]
    ]},
    { "type": "example", "title": "Batch Image Generation with DALL-E (Parallel)", "code": "const generateBatchImages = async (prompts, concurrency = 5) => {\n  const results = [];\n  // Process in chunks to respect rate limits\n  for (let i = 0; i < prompts.length; i += concurrency) {\n    const chunk = prompts.slice(i, i + concurrency);\n    const promises = chunk.map(prompt =>\n      openai.images.generate({\n        model: 'dall-e-3',\n        prompt: prompt,\n        size: '1792x1024',\n        quality: 'hd',\n        n: 1\n      })\n    );\n    const chunkResults = await Promise.allSettled(promises);\n    results.push(...chunkResults);\n    // Respect rate limits: pause between chunks\n    if (i + concurrency < prompts.length) {\n      await sleep(2000);\n    }\n  }\n  return results;\n};\n\n// Generate 50 images in batches of 5\nconst allPrompts = scenes.map(s => s.visual_prompt);\nconst images = await generateBatchImages(allPrompts, 5);" },
    { "type": "h2", "text": "Batch Video Generation" },
    { "type": "p", "text": "Video generation is the most time-consuming and expensive stage. Batch video generation requires careful orchestration because each clip can take <strong>1-5 minutes to generate</strong>, and API rate limits are stricter than for images." },
    { "type": "table", "rows": [
      ["Tool", "Batch Method", "Max Concurrent", "Avg Time Per Clip", "Cost Per 5s Clip"],
      ["Runway Gen-3", "Async task submission + polling", "5 concurrent", "60-120 seconds", "$0.05 - $0.10"],
      ["Kling AI API", "Batch task queue", "3 concurrent", "90-180 seconds", "$0.03 - $0.08"],
      ["Pika API", "Sequential with webhook callbacks", "2 concurrent", "60-90 seconds", "$0.04 - $0.07"],
      ["Luma Dream Machine", "Async generation with status polling", "3 concurrent", "45-90 seconds", "$0.03 - $0.06"],
      ["Haiper API", "Batch submission endpoint", "5 concurrent", "30-60 seconds", "$0.02 - $0.05"]
    ]},
    { "type": "example", "title": "Batch Video Generation with Polling", "code": "const batchGenerateVideos = async (imageUrls, prompts) => {\n  // Step 1: Submit all generation tasks\n  const tasks = [];\n  for (let i = 0; i < imageUrls.length; i++) {\n    const task = await submitVideoTask(imageUrls[i], prompts[i]);\n    tasks.push({ id: task.id, scene: i, status: 'processing' });\n    await sleep(1000); // Stagger submissions\n  }\n\n  // Step 2: Poll for completion\n  const completed = [];\n  while (completed.length < tasks.length) {\n    for (const task of tasks) {\n      if (task.status === 'processing') {\n        const status = await checkTaskStatus(task.id);\n        if (status.state === 'completed') {\n          task.status = 'completed';\n          task.videoUrl = status.output_url;\n          completed.push(task);\n          console.log(`Scene ${task.scene} complete (${completed.length}/${tasks.length})`);\n        } else if (status.state === 'failed') {\n          task.status = 'failed';\n          console.error(`Scene ${task.scene} failed: ${status.error}`);\n          // Resubmit failed task\n          const retry = await submitVideoTask(imageUrls[task.scene], prompts[task.scene]);\n          task.id = retry.id;\n          task.status = 'processing';\n        }\n      }\n    }\n    await sleep(10000); // Poll every 10 seconds\n  }\n  return completed;\n};" },
    { "type": "h2", "text": "Parallel Processing Strategies" },
    { "type": "image", "src": "https://placehold.co/800x450/1a1a2e/04AA6D?text=Processing+Strategies&font=raleway", "alt": "Diagram showing sequential vs parallel vs pipelined processing strategies", "caption": "Three approaches to batch processing: sequential (slow), parallel (fast but expensive), and pipelined (balanced)" },
    { "type": "p", "text": "There are three main strategies for processing batches, each with different tradeoffs:" },
    { "type": "p", "text": "<strong>1. Sequential:</strong> Process one item at a time. Slowest but simplest, uses minimal API quota. Best when rate limits are very strict or you need to use the output of one item as input for the next." },
    { "type": "p", "text": "<strong>2. Parallel:</strong> Process all items simultaneously. Fastest but can hit rate limits quickly and costs more due to burst pricing. Best for small batches with generous rate limits." },
    { "type": "p", "text": "<strong>3. Pipelined:</strong> Start the next item as soon as the previous one moves to the next stage. For example, while Scene 3 images are generating, Scene 2 is already in video generation, and Scene 1 is already in voiceover. This is the most efficient approach for end-to-end pipelines." },
    { "type": "table", "rows": [
      ["Strategy", "Speed", "API Usage", "Complexity", "Best For"],
      ["Sequential", "Slow", "Minimal", "Low", "Strict rate limits, dependent tasks"],
      ["Parallel", "Fast", "Burst (high)", "Medium", "Small batches, generous quotas"],
      ["Pipelined", "Optimized", "Steady", "High", "Full pipeline runs, production systems"],
      ["Chunked Parallel", "Balanced", "Controlled", "Medium", "Large batches with moderate rate limits"]
    ]},
    { "type": "h2", "text": "Managing API Rate Limits" },
    { "type": "p", "text": "Every AI API enforces rate limits — restrictions on how many requests you can make per minute, per hour, or per day. Batch processing must respect these limits or your requests will be rejected with <code>429 Too Many Requests</code> errors." },
    { "type": "table", "rows": [
      ["API", "Requests/Min (RPM)", "Tokens/Min (TPM)", "Images/Min", "Strategy"],
      ["OpenAI GPT-4", "500 RPM (Tier 3)", "80,000 TPM", "N/A", "Token-aware batching"],
      ["OpenAI DALL-E 3", "7 RPM (Tier 1) / 15 RPM (Tier 3)", "N/A", "7/15 per min", "Staggered with delays"],
      ["Runway", "~10 concurrent tasks", "N/A", "N/A", "Queue-based polling"],
      ["ElevenLabs", "100 RPM (Starter)", "N/A", "N/A", "Chunk by character count"],
      ["Stability AI", "150 RPM", "N/A", "150 per min", "Generous — parallel safe"]
    ]},
    { "type": "example", "title": "Rate Limiter Implementation", "code": "class RateLimiter {\n  constructor(maxRequests, windowMs) {\n    this.maxRequests = maxRequests;\n    this.windowMs = windowMs;\n    this.requests = [];\n  }\n\n  async waitForSlot() {\n    const now = Date.now();\n    // Remove expired timestamps\n    this.requests = this.requests.filter(t => now - t < this.windowMs);\n    if (this.requests.length >= this.maxRequests) {\n      const oldestRequest = this.requests[0];\n      const waitTime = this.windowMs - (now - oldestRequest) + 100;\n      console.log(`Rate limit reached. Waiting ${waitTime}ms...`);\n      await sleep(waitTime);\n    }\n    this.requests.push(Date.now());\n  }\n}\n\n// Usage: Max 7 DALL-E requests per 60 seconds\nconst dalleRateLimiter = new RateLimiter(7, 60000);\n\nfor (const prompt of imagePrompts) {\n  await dalleRateLimiter.waitForSlot();\n  const image = await generateImage(prompt);\n  results.push(image);\n}" },
    { "type": "h2", "text": "Cost Optimization for Bulk Generation" },
    { "type": "p", "text": "When processing hundreds of items, small cost differences per item compound quickly. A $0.03 difference per image across 1,000 images is $30. Optimizing costs involves choosing the right model tier, using batch APIs, and caching results." },
    { "type": "table", "rows": [
      ["Optimization", "Savings", "How It Works"],
      ["OpenAI Batch API", "50% off standard pricing", "Submit batch files, results within 24 hours"],
      ["Lower quality tiers", "30-60% off", "Use 'standard' instead of 'hd' for non-hero images"],
      ["Caching duplicates", "100% for cached items", "Store and reuse identical prompt results"],
      ["Off-peak generation", "Varies", "Some APIs have lower costs during off-peak hours"],
      ["Smaller models", "40-70% off", "Use GPT-3.5 for simple tasks instead of GPT-4"],
      ["Batch voiceover", "20-30% off", "Send longer text in fewer API calls"]
    ]},
    { "type": "example", "title": "OpenAI Batch API Usage", "code": "// Step 1: Create a JSONL batch file\nconst batchRequests = topics.map((topic, i) => ({\n  custom_id: `script-${i}`,\n  method: 'POST',\n  url: '/v1/chat/completions',\n  body: {\n    model: 'gpt-4',\n    messages: [\n      { role: 'system', content: 'Write a 60-second video script...' },\n      { role: 'user', content: `Topic: ${topic}` }\n    ]\n  }\n}));\n\n// Step 2: Upload batch file\nconst file = await openai.files.create({\n  file: createBatchFile(batchRequests),\n  purpose: 'batch'\n});\n\n// Step 3: Create batch job (50% cheaper than real-time)\nconst batch = await openai.batches.create({\n  input_file_id: file.id,\n  endpoint: '/v1/chat/completions',\n  completion_window: '24h'\n});\n\n// Step 4: Poll for results\n// Results are available within 24 hours at half the cost" },
    { "type": "h2", "text": "Quality Control at Scale" },
    { "type": "p", "text": "Generating at scale means more opportunities for errors. Quality control must be <strong>automated and systematic</strong> — you cannot manually review every image when you are generating 500 per day." },
    { "type": "p", "text": "<strong>Automated quality checks include:</strong>" },
    { "type": "p", "text": "- <strong>Image resolution validation:</strong> Confirm output dimensions match expected aspect ratio" },
    { "type": "p", "text": "- <strong>Content safety filtering:</strong> Run outputs through content moderation APIs to catch inappropriate content" },
    { "type": "p", "text": "- <strong>Similarity scoring:</strong> Compare generated images against a style reference using CLIP embeddings to ensure visual consistency" },
    { "type": "p", "text": "- <strong>Audio quality checks:</strong> Validate voiceover duration matches script timing, check for silence gaps or clipping" },
    { "type": "p", "text": "- <strong>Video integrity:</strong> Verify clip duration, file size, codec compatibility before assembly" },
    { "type": "example", "title": "Automated Quality Check Function", "code": "const qualityCheck = async (asset, type) => {\n  const checks = {\n    image: async (img) => {\n      const metadata = await sharp(img).metadata();\n      return {\n        passed: metadata.width >= 1792 && metadata.height >= 1024,\n        reason: metadata.width < 1792 ? 'Resolution too low' : 'OK',\n        dimensions: `${metadata.width}x${metadata.height}`\n      };\n    },\n    video: async (vid) => {\n      const probe = await ffprobe(vid);\n      const duration = parseFloat(probe.streams[0].duration);\n      return {\n        passed: duration >= 4.5 && duration <= 10.5,\n        reason: duration < 4.5 ? 'Too short' : duration > 10.5 ? 'Too long' : 'OK',\n        duration: `${duration}s`\n      };\n    },\n    audio: async (aud) => {\n      const probe = await ffprobe(aud);\n      const duration = parseFloat(probe.streams[0].duration);\n      const bitrate = parseInt(probe.streams[0].bit_rate);\n      return {\n        passed: bitrate >= 128000,\n        reason: bitrate < 128000 ? 'Bitrate too low' : 'OK',\n        duration: `${duration}s`\n      };\n    }\n  };\n  return checks[type](asset);\n};" },
    { "type": "note", "text": "Set up a dashboard (Grafana, a simple web page, or even a Google Sheet) that tracks batch job metrics: success rate, average generation time, cost per item, and quality check pass rate. This visibility is essential for optimizing at scale." },
    { "type": "exercise", "question": "What is the primary advantage of using OpenAI's Batch API over real-time API calls?", "options": ["Faster response times", "50% cost reduction", "Higher quality outputs", "No rate limits"], "answer": 1 },
    { "type": "exercise", "question": "Which processing strategy starts the next pipeline stage for an item as soon as the current stage completes, even while other items are still processing?", "options": ["Sequential processing", "Parallel processing", "Pipelined processing", "Random processing"], "answer": 2 },
    { "type": "exercise", "question": "What HTTP status code indicates you have exceeded an API's rate limit?", "options": ["400 Bad Request", "403 Forbidden", "429 Too Many Requests", "500 Internal Server Error"], "answer": 2 }
  ]
}