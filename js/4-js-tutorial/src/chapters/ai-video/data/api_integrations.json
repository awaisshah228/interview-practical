{
  "id": "api_integrations",
  "title": "API Integrations",
  "group": "AI Automation",
  "content": [
    { "type": "h1", "text": "API Integrations for AI Video" },
    { "type": "image", "src": "https://placehold.co/800x450/1a1a2e/04AA6D?text=API+Integrations&font=raleway", "alt": "Network diagram showing multiple AI APIs connected through a central orchestration layer", "caption": "API integrations connect specialized AI services into a unified video production system" },
    { "type": "h2", "text": "Why APIs Are the Foundation of Automation" },
    { "type": "p", "text": "Every AI tool you use through a web interface has an <strong>API (Application Programming Interface)</strong> behind it. APIs let your code talk directly to AI services — sending prompts, receiving outputs, and chaining services together without opening a single browser tab." },
    { "type": "p", "text": "In an automated video pipeline, APIs replace manual interactions: instead of typing a prompt into ChatGPT's web interface, your code sends a POST request to the OpenAI API. Instead of dragging an image into Runway's upload box, your code sends the image URL to Runway's API endpoint." },
    { "type": "note", "text": "API access often requires a separate subscription from the web interface. For example, having a ChatGPT Plus subscription does not give you OpenAI API access — you need to set up an API account at platform.openai.com with separate billing." },
    { "type": "h2", "text": "Authentication and API Keys" },
    { "type": "p", "text": "Every API requires authentication — proof that you have permission to use it. The most common method is an <strong>API key</strong>: a long string of characters you include in each request's headers." },
    { "type": "table", "rows": [
      ["Auth Method", "How It Works", "Used By"],
      ["API Key (Bearer Token)", "Send key in Authorization header", "OpenAI, Runway, Stability AI, ElevenLabs"],
      ["OAuth 2.0", "Token exchange flow, requires user consent", "YouTube, Instagram, TikTok, Google services"],
      ["API Key (Query Param)", "Append key to URL as parameter", "Some legacy APIs, Google Maps"],
      ["JWT (JSON Web Token)", "Self-signed token with claims", "Firebase, custom backends"]
    ]},
    { "type": "example", "title": "Secure API Key Management", "code": "// NEVER hardcode API keys in your source code.\n// Use environment variables loaded from a .env file.\n\n// .env file (add to .gitignore!):\n// OPENAI_API_KEY=sk-proj-abc123...\n// RUNWAY_API_KEY=rw_key_xyz789...\n// ELEVENLABS_API_KEY=el_key_def456...\n// STABILITY_API_KEY=sk-stab-ghi012...\n\n// Load in Node.js:\nimport 'dotenv/config';\n\nconst openaiKey = process.env.OPENAI_API_KEY;\nconst runwayKey = process.env.RUNWAY_API_KEY;\n\n// Use in requests:\nconst headers = {\n  'Authorization': `Bearer ${openaiKey}`,\n  'Content-Type': 'application/json'\n};" },
    { "type": "h2", "text": "OpenAI API (GPT + DALL-E)" },
    { "type": "p", "text": "The OpenAI API is typically the first service in any video pipeline. GPT handles <strong>script generation, metadata creation, and content planning</strong>, while DALL-E handles <strong>image generation</strong>." },
    { "type": "table", "rows": [
      ["Endpoint", "Purpose", "Model", "Cost (approx)"],
      ["/v1/chat/completions", "Script generation, metadata, planning", "gpt-4, gpt-4o, gpt-3.5-turbo", "$0.01-$0.03 per 1K tokens"],
      ["/v1/images/generations", "Scene image creation", "dall-e-3, dall-e-2", "$0.04-$0.08 per image (HD)"],
      ["/v1/audio/speech", "Text-to-speech narration", "tts-1, tts-1-hd", "$0.015 per 1K chars"],
      ["/v1/audio/transcriptions", "Transcribe audio for captions", "whisper-1", "$0.006 per minute"],
      ["/v1/embeddings", "Content similarity matching", "text-embedding-3-small", "$0.00002 per 1K tokens"]
    ]},
    { "type": "example", "title": "OpenAI GPT: Script Generation", "code": "import OpenAI from 'openai';\nconst openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\nconst generateScript = async (topic, duration = 60) => {\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4o',\n    messages: [\n      {\n        role: 'system',\n        content: `You are a professional video scriptwriter. Write a ${duration}-second video script. Return a JSON object with: title, total_duration, and scenes array. Each scene has: scene_number, narration, visual_prompt (for AI image generation), duration (seconds), transition (fade/cut/dissolve).`\n      },\n      {\n        role: 'user',\n        content: `Write a script about: ${topic}`\n      }\n    ],\n    response_format: { type: 'json_object' },\n    temperature: 0.8,\n    max_tokens: 2000\n  });\n  return JSON.parse(response.choices[0].message.content);\n};\n\nconst script = await generateScript('How Runway Gen-3 is changing video production');" },
    { "type": "example", "title": "OpenAI DALL-E: Image Generation", "code": "const generateSceneImage = async (visualPrompt) => {\n  const response = await openai.images.generate({\n    model: 'dall-e-3',\n    prompt: `${visualPrompt}. Cinematic, 4K, photorealistic, 16:9 aspect ratio, no text or watermarks.`,\n    size: '1792x1024',    // Closest to 16:9\n    quality: 'hd',         // Higher detail ($0.080 vs $0.040)\n    style: 'natural',      // 'vivid' for more dramatic, 'natural' for realistic\n    n: 1\n  });\n  return {\n    url: response.data[0].url,\n    revised_prompt: response.data[0].revised_prompt\n  };\n};" },
    { "type": "h2", "text": "Runway API (Image-to-Video & Text-to-Video)" },
    { "type": "p", "text": "Runway's API provides <strong>image-to-video and text-to-video generation</strong> — the core of turning static AI images into dynamic video clips. Gen-3 Alpha Turbo is the recommended model for automated pipelines due to its balance of quality and speed." },
    { "type": "example", "title": "Runway Gen-3: Image-to-Video", "code": "const generateRunwayVideo = async (imageUrl, motionPrompt) => {\n  // Step 1: Create generation task\n  const createResponse = await fetch('https://api.runwayml.com/v1/image_to_video', {\n    method: 'POST',\n    headers: {\n      'Authorization': `Bearer ${process.env.RUNWAY_API_KEY}`,\n      'X-Runway-Version': '2024-11-06',\n      'Content-Type': 'application/json'\n    },\n    body: JSON.stringify({\n      model: 'gen3a_turbo',\n      promptImage: imageUrl,\n      promptText: motionPrompt,\n      duration: 5,          // 5 or 10 seconds\n      ratio: '16:9',\n      watermark: false\n    })\n  });\n  const { id: taskId } = await createResponse.json();\n\n  // Step 2: Poll for completion\n  let task;\n  do {\n    await sleep(10000); // Check every 10s\n    const statusResponse = await fetch(\n      `https://api.runwayml.com/v1/tasks/${taskId}`,\n      { headers: { 'Authorization': `Bearer ${process.env.RUNWAY_API_KEY}` } }\n    );\n    task = await statusResponse.json();\n  } while (task.status === 'RUNNING');\n\n  if (task.status === 'SUCCEEDED') {\n    return task.output[0]; // Video URL\n  }\n  throw new Error(`Runway generation failed: ${task.failure}`);\n};" },
    { "type": "h2", "text": "ElevenLabs API (Voice & Sound Effects)" },
    { "type": "p", "text": "ElevenLabs provides the highest-quality <strong>text-to-speech</strong> available via API, along with voice cloning and AI sound effects. In video pipelines, it generates narration audio from scripts." },
    { "type": "table", "rows": [
      ["Endpoint", "Purpose", "Key Parameters"],
      ["/v1/text-to-speech/{voice_id}", "Generate speech from text", "voice_id, model_id, text, voice_settings"],
      ["/v1/text-to-speech/{voice_id}/stream", "Stream audio in real-time", "Same + output_format"],
      ["/v1/sound-generation", "AI sound effects", "text (description of sound), duration_seconds"],
      ["/v1/voices", "List available voices", "None (GET request)"],
      ["/v1/voices/add", "Clone a voice from audio samples", "name, files (audio samples)"]
    ]},
    { "type": "example", "title": "ElevenLabs: Text-to-Speech for Narration", "code": "const generateVoiceover = async (text, voiceId = 'pNInz6obpgDQGcFmaJgB') => {\n  // voiceId 'pNInz6obpgDQGcFmaJgB' = \"Adam\" (deep, narrator voice)\n  const response = await fetch(\n    `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`,\n    {\n      method: 'POST',\n      headers: {\n        'xi-api-key': process.env.ELEVENLABS_API_KEY,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        text: text,\n        model_id: 'eleven_multilingual_v2', // Best quality\n        voice_settings: {\n          stability: 0.5,        // 0-1: lower = more expressive\n          similarity_boost: 0.75, // 0-1: higher = closer to original voice\n          style: 0.3,            // 0-1: style exaggeration\n          use_speaker_boost: true\n        },\n        output_format: 'mp3_44100_128' // High quality MP3\n      })\n    }\n  );\n\n  // Response is raw audio bytes\n  const audioBuffer = await response.arrayBuffer();\n  fs.writeFileSync('voiceover.mp3', Buffer.from(audioBuffer));\n  return 'voiceover.mp3';\n};" },
    { "type": "example", "title": "ElevenLabs: AI Sound Effects", "code": "const generateSoundEffect = async (description, duration = 5) => {\n  const response = await fetch(\n    'https://api.elevenlabs.io/v1/sound-generation',\n    {\n      method: 'POST',\n      headers: {\n        'xi-api-key': process.env.ELEVENLABS_API_KEY,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        text: description,         // e.g., \"dramatic cinematic whoosh transition\"\n        duration_seconds: duration,\n        prompt_influence: 0.5      // 0-1: how closely to follow the description\n      })\n    }\n  );\n  const audioBuffer = await response.arrayBuffer();\n  fs.writeFileSync('sfx.mp3', Buffer.from(audioBuffer));\n  return 'sfx.mp3';\n};\n\n// Generate transition sound effects\nawait generateSoundEffect('cinematic whoosh transition sound', 2);\nawait generateSoundEffect('gentle ambient background music, technology theme', 60);" },
    { "type": "h2", "text": "Stability AI API" },
    { "type": "p", "text": "Stability AI offers image generation through Stable Diffusion models, image upscaling, inpainting, and outpainting. Their API is <strong>significantly cheaper</strong> than DALL-E for bulk generation." },
    { "type": "example", "title": "Stability AI: Image Generation", "code": "const generateStabilityImage = async (prompt) => {\n  const response = await fetch(\n    'https://api.stability.ai/v2beta/stable-image/generate/sd3',\n    {\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${process.env.STABILITY_API_KEY}`,\n        'Accept': 'image/*'\n      },\n      body: (() => {\n        const formData = new FormData();\n        formData.append('prompt', prompt);\n        formData.append('output_format', 'png');\n        formData.append('aspect_ratio', '16:9');\n        formData.append('model', 'sd3.5-large');  // Best quality\n        formData.append('negative_prompt', 'text, watermark, blurry, low quality');\n        return formData;\n      })()\n    }\n  );\n  const imageBuffer = await response.arrayBuffer();\n  return Buffer.from(imageBuffer);\n};" },
    { "type": "h2", "text": "Google Cloud Video AI" },
    { "type": "p", "text": "Google Cloud Video Intelligence API does not generate videos — instead, it <strong>analyzes existing videos</strong>. In automation pipelines, it is used for quality control: detecting scene changes, transcribing speech, identifying objects, and flagging inappropriate content." },
    { "type": "table", "rows": [
      ["Feature", "Use in Pipeline", "API Method"],
      ["Label Detection", "Verify video content matches intent", "annotate (LABEL_DETECTION)"],
      ["Shot Detection", "Validate transitions and scene changes", "annotate (SHOT_CHANGE_DETECTION)"],
      ["Speech Transcription", "Generate captions automatically", "annotate (SPEECH_TRANSCRIPTION)"],
      ["Explicit Content Detection", "Quality gate before publishing", "annotate (EXPLICIT_CONTENT_DETECTION)"],
      ["Object Tracking", "Verify visual consistency across scenes", "annotate (OBJECT_TRACKING)"]
    ]},
    { "type": "h2", "text": "API Comparison Table" },
    { "type": "p", "text": "Choosing the right API for each pipeline stage depends on quality requirements, speed, cost, and rate limits. Here is a comprehensive comparison." },
    { "type": "table", "rows": [
      ["Category", "API", "Quality", "Speed", "Cost (Low End)", "Rate Limit"],
      ["Text/Script", "OpenAI GPT-4o", "Excellent", "Fast (2-5s)", "$0.005/1K tokens", "500 RPM"],
      ["Text/Script", "Anthropic Claude", "Excellent", "Fast (2-5s)", "$0.003/1K tokens", "50 RPM (varies)"],
      ["Images", "OpenAI DALL-E 3", "Very Good", "10-15s", "$0.04/image", "7-15 RPM"],
      ["Images", "Stability AI SD3.5", "Good", "5-10s", "$0.002/image", "150 RPM"],
      ["Images", "Midjourney (unofficial)", "Excellent", "30-60s", "$0.01/image", "Varies"],
      ["Video", "Runway Gen-3", "Excellent", "60-120s", "$0.05/clip", "10 concurrent"],
      ["Video", "Kling AI", "Very Good", "90-180s", "$0.03/clip", "3 concurrent"],
      ["Voice", "ElevenLabs", "Excellent", "2-5s", "$0.18/1K chars", "100 RPM"],
      ["Voice", "OpenAI TTS", "Good", "1-3s", "$0.015/1K chars", "50 RPM"],
      ["Analysis", "Google Video AI", "Excellent", "30-120s", "$0.10/minute", "600 RPM"]
    ]},
    { "type": "h2", "text": "Rate Limiting and Error Handling" },
    { "type": "p", "text": "Robust API integration requires handling failures gracefully. The three most common failures are: <strong>rate limit errors (429)</strong>, <strong>server errors (500/503)</strong>, and <strong>timeout errors</strong>." },
    { "type": "example", "title": "Resilient API Caller with Retry Logic", "code": "const callAPIWithRetry = async (apiCall, options = {}) => {\n  const { maxRetries = 3, baseDelay = 1000, maxDelay = 30000 } = options;\n\n  for (let attempt = 0; attempt <= maxRetries; attempt++) {\n    try {\n      const result = await apiCall();\n      return result;\n    } catch (error) {\n      const isRateLimit = error.status === 429;\n      const isServerError = error.status >= 500;\n      const isRetryable = isRateLimit || isServerError;\n\n      if (!isRetryable || attempt === maxRetries) {\n        throw error;\n      }\n\n      // Use Retry-After header if available, otherwise exponential backoff\n      let delay;\n      if (isRateLimit && error.headers?.['retry-after']) {\n        delay = parseInt(error.headers['retry-after']) * 1000;\n      } else {\n        delay = Math.min(baseDelay * Math.pow(2, attempt), maxDelay);\n      }\n\n      console.warn(\n        `API call failed (${error.status}). ` +\n        `Retry ${attempt + 1}/${maxRetries} in ${delay}ms...`\n      );\n      await sleep(delay);\n    }\n  }\n};\n\n// Usage:\nconst script = await callAPIWithRetry(\n  () => openai.chat.completions.create({ model: 'gpt-4o', messages: [...] }),\n  { maxRetries: 3, baseDelay: 2000 }\n);" },
    { "type": "h2", "text": "Webhook Integrations" },
    { "type": "p", "text": "Webhooks allow APIs to <strong>push notifications to your server</strong> when an event occurs, instead of you polling for status updates. This is more efficient and faster for long-running tasks like video generation." },
    { "type": "example", "title": "Webhook Receiver for Runway Completion", "code": "import express from 'express';\nconst app = express();\n\n// Webhook endpoint that Runway calls when video generation completes\napp.post('/webhooks/runway', express.json(), async (req, res) => {\n  const { task_id, status, output } = req.body;\n\n  if (status === 'SUCCEEDED') {\n    console.log(`Video ready: ${output[0]}`);\n    // Continue pipeline: download video, assemble, publish\n    await downloadVideo(output[0], `clips/${task_id}.mp4`);\n    await triggerAssemblyStage(task_id);\n  } else if (status === 'FAILED') {\n    console.error(`Task ${task_id} failed: ${req.body.failure}`);\n    await retryOrNotify(task_id);\n  }\n\n  res.status(200).json({ received: true });\n});\n\napp.listen(3000, () => console.log('Webhook server listening on port 3000'));\n\n// When creating a Runway task, specify the webhook URL:\n// { ...params, webhook: 'https://your-server.com/webhooks/runway' }" },
    { "type": "note", "text": "For webhook-based architectures, use a tool like ngrok during development to expose your local server to the internet. In production, deploy your webhook receiver to a cloud service with HTTPS (AWS Lambda, Vercel, Railway)." },
    { "type": "exercise", "question": "Which authentication method requires a token exchange flow with user consent?", "options": ["API Key (Bearer Token)", "OAuth 2.0", "JWT", "API Key (Query Param)"], "answer": 1 },
    { "type": "exercise", "question": "Which API is best suited for bulk image generation when cost is the primary concern?", "options": ["OpenAI DALL-E 3 ($0.04/image)", "Stability AI SD3.5 ($0.002/image)", "Midjourney ($0.01/image)", "ElevenLabs (voice only)"], "answer": 1 },
    { "type": "exercise", "question": "What is the advantage of webhooks over polling for long-running API tasks?", "options": ["Webhooks are cheaper per API call", "Webhooks push notifications when complete instead of requiring repeated status checks", "Webhooks generate higher quality outputs", "Webhooks bypass rate limits"], "answer": 1 }
  ]
}